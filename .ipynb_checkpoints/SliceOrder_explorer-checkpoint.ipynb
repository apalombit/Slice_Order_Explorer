{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-driven inference of slice ordering / timing (resting state fMRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook implements a PoC that verify whether the slice ordering / timing in use make sense data-wise.\n",
    "#\n",
    "# It is based on the idea that if the slice timing / order is the right one, then the local signal coherency \n",
    "# should be maximal (following the slice timing correction idea). To measure local coherency, one can use\n",
    "# the regional homogeneity (ReHo). Here this was tested with local NN filter of 27 neighbours or, to emphasise\n",
    "# out-of-slice contributions a 18 option (mask to measure only correlation with neighours on adjacent slices)\n",
    "# where a reasonable correlation should be restored by correct slice timing (at least in functionally homogenous ROI).\n",
    "#\n",
    "# This migth be useful when metadata (e.g dicom header tags) are not encoding this info reliably and\n",
    "# is not immediately clear from the signals upon visual inspection.\n",
    "# It allows for consistent automation of the ordering inference to also check whether a dataset made use of \n",
    "# uniform slice ordering in all scans.\n",
    "#\n",
    "# All six canonical order are being applied to each rs-fMRI dataset with *minimal* pre-processing. \n",
    "# Once cleaned, the ReHo is extracted from each dataset and its average value should be highest when using the correct\n",
    "# slice ordering -> using correct pattern should recover best local signal coherence.\n",
    "# \n",
    "# So even if the slice timing used for scanning is unclear or not reported (e.g. Philips or old GE) with this \n",
    "# you might be able to infer the most likely pattern alternatives used in a dataset.\n",
    "# \n",
    "# Absolutely no warranty on functionality, tested on freely available datasets from OpenfMRI project (http://www.openfmri.org)\n",
    "# on ds000133, ds000172, ds000201, ds000220, ds000245, ds000256. The approach is functional in all tested cases.\n",
    "#\n",
    "# Note:\n",
    "# 1 - it was found to always work to automatically detect whether interleaved rather than sequential was used\n",
    "# 2 - it detects mixed cases of patterns within same dataset\n",
    "# 3 - inference of direction (ascending / descending) less reliable if data is noisy or with lots of motion ( -5 helps)\n",
    "# 4 - inference of correct slice order from short TR scans works for main pattern but direction less clear (TR < 1.5 sec) \n",
    "#     ...  might be not relevant correction anyway ?\n",
    "# 5 - improving the stats might be less effective than using context data (ie. what is the scanner/model/slices/...)\n",
    "# \n",
    "# PS: you need to have fsl, nipype and so on available locally !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "\n",
    "from nipype import Node, Workflow\n",
    "from nipype.interfaces.fsl import SliceTimer, MCFLIRT, Smooth, ExtractROI\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer(MyList, tgtf):\n",
    "    MyFile=open(tgtf,'w')\n",
    "    MyList=map(lambda x:x+'\\n', MyList)\n",
    "    MyFile.writelines(MyList)\n",
    "    MyFile.close()\n",
    "\n",
    "def f_kendall(timeseries_matrix):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the Kendall's coefficient of concordance for a number of\n",
    "    time-series in the input matrix\n",
    "    Parameters\n",
    "    ----------\n",
    "    timeseries_matrix : ndarray\n",
    "        A matrix of ranks of a subset subject's brain voxels\n",
    "    Returns\n",
    "    -------\n",
    "    kcc : float\n",
    "        Kendall's coefficient of concordance on the given input matrix\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    nk = timeseries_matrix.shape\n",
    "\n",
    "    n = nk[0]\n",
    "    k = nk[1]\n",
    "\n",
    "    sr = np.sum(timeseries_matrix, 1)\n",
    "    sr_bar = np.mean(sr)\n",
    "    s = np.sum(np.power(sr, 2)) - n*np.power(sr_bar, 2)\n",
    "    kcc = 12 *s/np.power(k, 2)/(np.power(n, 3) - n)\n",
    "    return kcc\n",
    "\n",
    "def compute_reho(in_file, mask_file, cluster_size = 7, out_file = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the ReHo Map, by computing tied ranks of the timepoints,\n",
    "    followed by computing Kendall's coefficient concordance(KCC) of a\n",
    "    timeseries with its neighbours\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_file : nifti file\n",
    "        4D EPI File\n",
    "    mask_file : nifti file\n",
    "        Mask of the EPI File(Only Compute ReHo of voxels in the mask)\n",
    "    out_file : nifti file\n",
    "        Where to save result\n",
    "    cluster_size : integer\n",
    "        for a brain voxel the number of neighbouring brain voxels to use for\n",
    "        KCC.\n",
    "    Returns\n",
    "    -------\n",
    "    out_file : nifti file\n",
    "        ReHo map of the input EPI image\n",
    "    \"\"\"\n",
    "\n",
    "    res_fname = (in_file)\n",
    "    res_mask_fname = (mask_file)\n",
    "    CUTNUMBER = 10\n",
    "\n",
    "    if not (cluster_size == 27 or cluster_size == 19 or cluster_size == 7 or cluster_size == 18):\n",
    "        cluster_size = 27\n",
    "\n",
    "    nvoxel = cluster_size\n",
    "\n",
    "    res_img = nb.load(res_fname)\n",
    "    res_mask_img = nb.load(res_mask_fname)\n",
    "\n",
    "    res_data = res_img.get_data()\n",
    "    res_mask_data = res_mask_img.get_data()\n",
    "\n",
    "    print(res_data.shape)\n",
    "    (n_x, n_y, n_z, n_t) = res_data.shape\n",
    "\n",
    "    # \"flatten\" each volume of the timeseries into one big array instead of\n",
    "    # x,y,z - produces (timepoints, N voxels) shaped data array\n",
    "    res_data = np.reshape(res_data, (n_x*n_y*n_z, n_t), order='F').T\n",
    "\n",
    "    # create a blank array of zeroes of size n_voxels, one for each time point\n",
    "    Ranks_res_data = np.tile((np.zeros((1, (res_data.shape)[1]))),\n",
    "                             [(res_data.shape)[0], 1])\n",
    "\n",
    "    # divide the number of total voxels by the cutnumber (set to 10)\n",
    "    # ex. end up with a number in the thousands if there are tens of thousands\n",
    "    # of voxels\n",
    "    segment_length = np.ceil(float((res_data.shape)[1])/float(CUTNUMBER))\n",
    "\n",
    "    for icut in range(0, CUTNUMBER):\n",
    "\n",
    "        segment = None\n",
    "\n",
    "        # create a Numpy array of evenly spaced values from the segment\n",
    "        # starting point up until the segment_length integer\n",
    "        if not (icut == (CUTNUMBER - 1)):\n",
    "            segment = np.array(np.arange(icut * segment_length,\n",
    "                                         (icut+1) * segment_length))\n",
    "        else:\n",
    "            segment = np.array(np.arange(icut * segment_length,\n",
    "                                         (res_data.shape[1])))\n",
    "\n",
    "        segment = np.int64(segment[np.newaxis])\n",
    "\n",
    "        # res_data_piece is a chunk of the original timeseries in_file, but\n",
    "        # aligned with the current segment index spacing\n",
    "        res_data_piece = res_data[:, segment[0]]\n",
    "        nvoxels_piece = res_data_piece.shape[1]\n",
    "\n",
    "        # run a merge sort across the time axis, re-ordering the flattened\n",
    "        # volume voxel arrays\n",
    "        res_data_sorted = np.sort(res_data_piece, 0, kind='mergesort')\n",
    "        sort_index = np.argsort(res_data_piece, axis=0, kind='mergesort')\n",
    "\n",
    "        # subtract each volume from each other\n",
    "        db = np.diff(res_data_sorted, 1, 0)\n",
    "\n",
    "        # convert any zero voxels into \"True\" flag\n",
    "        db = db == 0\n",
    "\n",
    "        # return an n_voxel (n voxels within the current segment) sized array\n",
    "        # of values, each value being the sum total of TRUE values in \"db\"\n",
    "        sumdb = np.sum(db, 0)\n",
    "\n",
    "        temp_array = np.array(np.arange(0, n_t))\n",
    "        temp_array = temp_array[:, np.newaxis]\n",
    "\n",
    "        sorted_ranks = np.tile(temp_array, [1, nvoxels_piece])\n",
    "\n",
    "        if np.any(sumdb[:]):\n",
    "\n",
    "            tie_adjust_index = np.flatnonzero(sumdb)\n",
    "\n",
    "            for i in range(0, len(tie_adjust_index)):\n",
    "\n",
    "                ranks = sorted_ranks[:, tie_adjust_index[i]]\n",
    "\n",
    "                ties = db[:, tie_adjust_index[i]]\n",
    "\n",
    "                tieloc = np.append(np.flatnonzero(ties), n_t + 2)\n",
    "                maxties = len(tieloc)\n",
    "                tiecount = 0\n",
    "\n",
    "                while(tiecount < maxties -1):\n",
    "                    tiestart = tieloc[tiecount]\n",
    "                    ntied = 2\n",
    "                    while(tieloc[tiecount + 1] == (tieloc[tiecount] + 1)):\n",
    "                        tiecount += 1\n",
    "                        ntied += 1\n",
    "\n",
    "                    ranks[tiestart:tiestart + ntied] = np.ceil(np.float32(np.sum(ranks[tiestart:tiestart + ntied ]))/np.float32(ntied))\n",
    "                    tiecount += 1\n",
    "\n",
    "                sorted_ranks[:, tie_adjust_index[i]] = ranks\n",
    "\n",
    "        del db, sumdb\n",
    "        sort_index_base = np.tile(np.multiply(np.arange(0, nvoxels_piece), n_t), [n_t, 1])\n",
    "        sort_index += sort_index_base\n",
    "        del sort_index_base\n",
    "\n",
    "        ranks_piece = np.zeros((n_t, nvoxels_piece))\n",
    "\n",
    "        ranks_piece = ranks_piece.flatten(order='F')\n",
    "        sort_index = sort_index.flatten(order='F')\n",
    "        sorted_ranks = sorted_ranks.flatten(order='F')\n",
    "\n",
    "        ranks_piece[sort_index] = np.array(sorted_ranks)\n",
    "\n",
    "        ranks_piece = np.reshape(ranks_piece, (n_t, nvoxels_piece), order='F')\n",
    "\n",
    "        del sort_index, sorted_ranks\n",
    "\n",
    "        Ranks_res_data[:, segment[0]] = ranks_piece\n",
    "\n",
    "        sys.stdout.write('.')\n",
    "\n",
    "    Ranks_res_data = np.reshape(Ranks_res_data, (n_t, n_x, n_y, n_z), order='F')\n",
    "\n",
    "    K = np.zeros((n_x, n_y, n_z))\n",
    "\n",
    "    mask_cluster = np.ones((3, 3, 3))\n",
    "\n",
    "    if nvoxel == 19:\n",
    "        mask_cluster[0, 0, 0] = 0\n",
    "        mask_cluster[0, 2, 0] = 0\n",
    "        mask_cluster[2, 0, 0] = 0\n",
    "        mask_cluster[2, 2, 0] = 0\n",
    "        mask_cluster[0, 0, 2] = 0\n",
    "        mask_cluster[0, 2, 2] = 0\n",
    "        mask_cluster[2, 0, 2] = 0\n",
    "        mask_cluster[2, 2, 2] = 0\n",
    "        \n",
    "    elif nvoxel == 18:\n",
    "        # null mid disk and disky-shaped\n",
    "        mask_cluster[0, 0, 0] = 0\n",
    "        mask_cluster[0, 2, 0] = 0\n",
    "        mask_cluster[2, 0, 0] = 0\n",
    "        mask_cluster[2, 2, 0] = 0\n",
    "        mask_cluster[0, 0, 2] = 0\n",
    "        mask_cluster[0, 2, 2] = 0\n",
    "        mask_cluster[2, 0, 2] = 0\n",
    "        mask_cluster[2, 2, 2] = 0\n",
    "        mask_cluster[1, 0, 0] = 0\n",
    "        mask_cluster[1, 0, 1] = 0\n",
    "        mask_cluster[1, 0, 2] = 0\n",
    "        mask_cluster[1, 2, 0] = 0\n",
    "        mask_cluster[1, 2, 1] = 0\n",
    "        mask_cluster[1, 2, 2] = 0\n",
    "        mask_cluster[1, 1, 0] = 0\n",
    "        mask_cluster[1, 1, 2] = 0\n",
    "\n",
    "    elif nvoxel == 7:\n",
    "\n",
    "        mask_cluster[0, 0, 0] = 0\n",
    "        mask_cluster[0, 1, 0] = 0\n",
    "        mask_cluster[0, 2, 0] = 0\n",
    "        mask_cluster[0, 0, 1] = 0\n",
    "        mask_cluster[0, 2, 1] = 0\n",
    "        mask_cluster[0, 0, 2] = 0\n",
    "        mask_cluster[0, 1, 2] = 0\n",
    "        mask_cluster[0, 2, 2] = 0\n",
    "        mask_cluster[1, 0, 0] = 0\n",
    "        mask_cluster[1, 2, 0] = 0\n",
    "        mask_cluster[1, 0, 2] = 0\n",
    "        mask_cluster[1, 2, 2] = 0\n",
    "        mask_cluster[2, 0, 0] = 0\n",
    "        mask_cluster[2, 1, 0] = 0\n",
    "        mask_cluster[2, 2, 0] = 0\n",
    "        mask_cluster[2, 0, 1] = 0\n",
    "        mask_cluster[2, 2, 1] = 0\n",
    "        mask_cluster[2, 0, 2] = 0\n",
    "        mask_cluster[2, 1, 2] = 0\n",
    "        mask_cluster[2, 2, 2] = 0\n",
    "\n",
    "    for i in range(1, n_x - 1):\n",
    "        for j in range(1, n_y -1):\n",
    "            for k in range(1, n_z -1):\n",
    "\n",
    "                block = Ranks_res_data[:, i-1:i+2, j-1:j+2, k-1:k+2]\n",
    "                mask_block = res_mask_data[i-1:i+2, j-1:j+2, k-1:k+2]\n",
    "\n",
    "                if not(int(mask_block[1, 1, 1]) == 0):\n",
    "\n",
    "                    if nvoxel == 19 or nvoxel == 7 or nvoxel == 18:\n",
    "                        mask_block = np.multiply(mask_block, mask_cluster)\n",
    "\n",
    "                    R_block = np.reshape(block, (block.shape[0], 27),\n",
    "                                         order='F')\n",
    "                    mask_R_block = R_block[:, np.argwhere(np.reshape(mask_block, (1, 27), order='F') > 0)[:, 1]]\n",
    "\n",
    "                    K[i, j, k] = f_kendall(mask_R_block)\n",
    "\n",
    "    img = nb.Nifti1Image(K, header=res_img.get_header(),\n",
    "                         affine=res_img.get_affine())\n",
    "    \n",
    "    if out_file is not None:\n",
    "        reho_file = out_file\n",
    "    else:\n",
    "        reho_file = os.path.join(os.getcwd(), 'ReHo.nii.gz')\n",
    "    img.to_filename(reho_file)\n",
    "    \n",
    "    return reho_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"/Volumes/G_drive/Backup_06062020/ds000256/\"\n",
    "rehopath   = base + \"/ReHo/\"\n",
    "order_path = base + \"/SlTi/\"\n",
    "\n",
    "sbjpatt  = \"\"\n",
    "sess     = \"func\"\n",
    "fmriname = \"_task-restbaseline_bold.nii.gz\"\n",
    "\n",
    "os.makedirs(rehopath, exist_ok=True)\n",
    "os.makedirs(order_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR    = 2.5  # repetition time (assiming no lags between volumes) [seconds]\n",
    "fwhm  = 3  # spatial filter size\n",
    "dummy = 10 # how many dummy vols to consider\n",
    "n_sl  = 32 # number of slices\n",
    "rh    = 18 # neighbourhood selection for ReHo: 27 for full NN, 18 for slice-sensitive mask\n",
    "\n",
    "n_procs = 8 # parallel processing of fMRI data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define slice ordering file for fsl correction (test cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slice ordering file based on declared #slices on 6 canionical patterns + random ones\n",
    "# https://en.wikibooks.org/wiki/SPM/Slice_Timing\n",
    "\n",
    "n_noise = 6 # how many \"noise\" permutation to use (totally random slice ordering)\n",
    "\n",
    "# seq asc 1 2 3 4\n",
    "slice_order = list(np.arange(1, n_sl+1).astype(str))\n",
    "writer(slice_order, order_path + 'slti_1.txt') \n",
    "\n",
    "# seq desc 4 3 2 1\n",
    "slice_order = list(reversed(list(np.arange(1, n_sl+1).astype(str))))\n",
    "writer(slice_order, order_path + 'slti_2.txt') \n",
    "\n",
    "# int asc 1 3 2 4\n",
    "slice_order = list(np.arange(1, n_sl+1, 2).astype(str)) + list(np.arange(2, n_sl+1, 2).astype(str))\n",
    "writer(slice_order, order_path + 'slti_3.txt') \n",
    "\n",
    "# int desc 4 2 3 1\n",
    "slice_order = list(reversed(list(np.arange(1, n_sl+1, 2).astype(str)) + list(np.arange(2, n_sl+1, 2).astype(str))))\n",
    "writer(slice_order, order_path + 'slti_4.txt') \n",
    "\n",
    "# int2 asc 2 4 1 3\n",
    "slice_order = list(np.arange(2, n_sl+1, 2).astype(str)) + list(np.arange(1, n_sl+1, 2).astype(str))\n",
    "writer(slice_order, order_path + 'slti_5.txt') \n",
    "\n",
    "# int2 dsc 3 1 4 2\n",
    "slice_order = list(reversed(list(np.arange(2, n_sl+1, 2).astype(str)) + list(np.arange(1, n_sl+1, 2).astype(str))))\n",
    "writer(slice_order, order_path + 'slti_6.txt') \n",
    "\n",
    "n_last = 7+n_noise\n",
    "for rr in np.arange(7,n_last):\n",
    "    slice_order = list(shuffle(np.arange(1, n_sl+1).astype(str), random_state=rr))\n",
    "    writer(slice_order, order_path + 'slti_{}.txt'.format(rr)) # random permutation of slices   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate ReHo maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fRMI by subejcts in different folders (BIDS-formatted)\n",
    "sbj_list = sorted([sbj.split(\"/\")[-1].replace(\"sub-\",\"\") for sbj in glob.glob(base + \"sub-{}*\".format(sbjpatt))])\n",
    "\n",
    "for sbj in sbj_list:\n",
    "    fmri_nii = base + \"sub-{}/{}/\".format(sbj, sess) + \"sub-{}{}\".format(sbj, fmriname) # BIDS-spec pattern\n",
    "    proc_ref   = '{}_preproc'.format(sbj)\n",
    "    extract    = Node(ExtractROI(t_min=dummy, t_size=-1, output_type='NIFTI_GZ'), name=\"extract\")\n",
    "    slicetimer = Node(SliceTimer(time_repetition=TR), name=\"slicetimer\")        \n",
    "    slicetimer.iterables = (\"custom_order\", [order_path + \"slti_{}.txt\".format(pp) for pp in np.arange(1,n_last)])\n",
    "    mcflirt    = Node(MCFLIRT(mean_vol=True, save_plots=True), name=\"mcflirt\")\n",
    "    smooth     = Node(Smooth(fwhm=fwhm), name=\"smooth\")\n",
    "    preproc01  = Workflow(name=proc_ref, base_dir=base)\n",
    "    preproc01.connect([(extract,    slicetimer, [('roi_file', 'in_file')]),\n",
    "                       (slicetimer, mcflirt,    [('slice_time_corrected_file', 'in_file')]),\n",
    "                       (mcflirt,    smooth,     [('out_file', 'in_file')])])\n",
    "    extract.inputs.in_file = fmri_nii\n",
    "    preproc01.run('MultiProc', plugin_args={'n_procs': n_procs})\n",
    "    \n",
    "    for opt in np.arange(1, n_last):\n",
    "        basepath = glob.glob(base + \"{}/_custom_order_*slti_{}.txt\".format(proc_ref, opt))[0] + \"/smooth/\".format(proc_ref)\n",
    "        proc_f   = basepath + fmri_nii.split(\"/\")[-1].replace(\".nii.gz\",\"\") + \"_roi_st_mcf_smooth.nii.gz\"\n",
    "        if opt == 1:\n",
    "            # calculate bet mask from first run and always use the same by subject in all slice timing tests\n",
    "            in_f  = basepath + \"meanvol\"\n",
    "            out_f = in_f + \"_bet\"\n",
    "            !fslmaths {proc_f} -Tmean {in_f}\n",
    "            !bet {in_f} {out_f} -m # source of fixed bias eventually \n",
    "        \n",
    "        compute_reho(proc_f, out_f + \"_mask.nii.gz\", rh, \n",
    "                     out_file = rehopath + \"/\" + sbj + \"_\" + str(opt) + \"_ReHo.nii.gz\")\n",
    "    \n",
    "    shutil.rmtree(base + \"/{}/\".format(proc_ref)) # cleanup entire folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extract some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all estimated ReHo maps\n",
    "rehos = [[ff.split(\"/\")[-1].split(\"_\")[0], ff.split(\"/\")[-1].split(\"_\")[1], ff] for ff in glob.glob(rehopath+\"*_ReHo.nii.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr = 0.05 # exclude null or near-null local correlation (likely just noisy ROIs)\n",
    "\n",
    "res = pd.DataFrame(columns=['sbj', 'ord', 'rehoavg', 'rehopct'])\n",
    "for nii in rehos:\n",
    "    img = nb.load(nii[-1]).get_fdata().ravel()\n",
    "    img = img[img>thr]\n",
    "    nmean, pct = np.nanmean(img), np.percentile(img, 90)\n",
    "    if int(nii[1]) < 7: # non-noise alternative slice orderings\n",
    "        ord_ = nii[1] # legit slice orders\n",
    "    else:\n",
    "        ord_ = \"0\" # noise cases\n",
    "    res = res.append({\"sbj\":nii[0], \"ord\":ord_, \"rehoavg\":nmean, \"rehopct\":pct}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which metric to use to determine the ReHo local stats (rehopct = 90th percentile of ReHo distribution)\n",
    "metric = \"rehopct\" \n",
    "\n",
    "signif = pd.DataFrame(columns=['sbj', 'ord', 'reho', 'tt'])\n",
    "for sbj in np.unique(res.sbj.values):\n",
    "    rsel = res[res.sbj == sbj].sort_values([\"rehopct\",\"rehoavg\"])\n",
    "    \n",
    "    for oo in np.arange(0,7):\n",
    "        # pseudo t-stat to assess the improvement of ReHo against random ordering used for correction\n",
    "        tt2 = (np.nanmean(rsel[rsel.ord == str(oo)][metric].values - np.nanmean(rsel[rsel.ord == \"0\"][metric].values))) / \\\n",
    "               np.nanstd(rsel[rsel.ord == \"0\"][metric].values)\n",
    "        \n",
    "        signif = signif.append({\"sbj\"  :sbj, \"ord\" : str(oo), \n",
    "                                \"reho\" :round(np.nanmean(rsel[rsel.ord == str(oo)][metric].values),3), \n",
    "                                \"tt\"   : round(np.abs(tt2), 3)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude impossible cases example for Siemens scanner the odd pattern can't be 5 or 6 etc.\n",
    "# see: https://en.wikibooks.org/wiki/SPM/Slice_Timing#Siemens_scanners\n",
    "signif = signif[(signif.ord != \"3\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = []\n",
    "for sbj in np.unique(res.sbj.values):\n",
    "    rsel = signif[signif.sbj == sbj].sort_values([\"reho\",\"sbj\"])\n",
    "    lls.append(rsel[rsel.sbj==sbj].iloc[-1:].ord.values[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.array(lls).astype(int).ravel()\n",
    "y  = np.bincount(x)\n",
    "ii = np.nonzero(y)[0]\n",
    "print(\"ordering option, counts\")\n",
    "np.vstack((ii,y[ii])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
